import os
import random
import shutil

import numpy as np
from PIL import Image
from tqdm import tqdm
from datetime import datetime

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from sklearn.model_selection import train_test_split

# å¯¼å…¥å•ç‹¬çš„æ¨¡å‹ç±»
from ResNetUNet import ResNetUNet

# ====================== 1. å…¨å±€é…ç½®ï¼ˆæ ¸å¿ƒå¯é…ç½®å‚æ•°ï¼‰ ======================
# åŸºç¡€è·¯å¾„é…ç½®ï¼ˆå·²ä¿®æ­£ä¸ºdatasetï¼‰
ROOT_DIR = "dataset"  # ä¿®æ­£ï¼šä»met_datasetæ”¹ä¸ºdataset
DATA_DIR = os.path.join(ROOT_DIR, "data")  # åŸå§‹æ•°æ®ç›®å½•
TRAIN_DIR = os.path.join(ROOT_DIR, "train")
VAL_DIR = os.path.join(ROOT_DIR, "val")
TEST_DIR = os.path.join(ROOT_DIR, "test")
TEST_OUT_DIR = os.path.join(ROOT_DIR, "test_results")
WEIGHT_DIR = "weight"  # æƒé‡ä¿å­˜ç›®å½•

# è®­ç»ƒ/æµ‹è¯•æ¨¡å¼æ§åˆ¶ï¼ˆtrain=è®­ç»ƒï¼Œtest=æµ‹è¯•ï¼‰
MODE = "train"
# æµ‹è¯•æ¨¡å¼ä¸‹æŒ‡å®šçš„æƒé‡æ–‡ä»¶è·¯å¾„
TEST_WEIGHT_PATH = "weight/20251227_174219_best_loss_0.0242_fp16.pth"  # æ›¿æ¢ä¸ºä½ çš„æƒé‡è·¯å¾„

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 12
EPOCHS = 50
LEARNING_RATE = 2e-4
TARGET_SIZE = (512, 512)  # è®­ç»ƒæ—¶ç»Ÿä¸€çš„ç›®æ ‡å°ºå¯¸
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MASK_THRESHOLD = 127  # æ©ç äºŒå€¼åŒ–é˜ˆå€¼
RANDOM_SEED = 42  # éšæœºç§å­ï¼ˆä¿è¯åˆ’åˆ†ç»“æœå¯å¤ç°ï¼‰


# ====================== 2. å·¥å…·å‡½æ•° ======================
class ResizeWithPad:
    """ä¿æŒå›¾ç‰‡æ¯”ä¾‹çš„resizeï¼Œä¸è¶³éƒ¨åˆ†å¡«å……é»‘è‰²"""

    def __init__(self, target_size):
        self.target_size = target_size  # (width, height)

    def __call__(self, img):
        original_w, original_h = img.size
        target_w, target_h = self.target_size

        scale = min(target_w / original_w, target_h / original_h)
        new_w = int(original_w * scale)
        new_h = int(original_h * scale)

        img = img.resize((new_w, new_h), Image.LANCZOS)

        pad_left = (target_w - new_w) // 2
        pad_top = (target_h - new_h) // 2
        pad_right = target_w - new_w - pad_left
        pad_bottom = target_h - new_h - pad_top

        padding = (pad_left, pad_top, pad_right, pad_bottom)
        img = transforms.Pad(padding, fill=0)(img)

        return img


def create_dirs():
    """åˆ›å»ºæ‰€éœ€ç›®å½•"""
    dirs = [
        TRAIN_DIR, VAL_DIR, TEST_DIR, WEIGHT_DIR,
        os.path.join(TRAIN_DIR, "images"), os.path.join(TRAIN_DIR, "masks"),
        os.path.join(VAL_DIR, "images"), os.path.join(VAL_DIR, "masks"),
        os.path.join(TEST_DIR, "images"), os.path.join(TEST_DIR, "masks")
    ]
    for d in dirs:
        os.makedirs(d, exist_ok=True)


def split_dataset_from_data():
    """
    ä»dataç›®å½•åˆ’åˆ†æ•°æ®é›†ï¼š
    1. å…ˆæ¸…ç©ºval/testç›®å½•å¹¶å°†æ–‡ä»¶ç§»å›dataï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    2. ä»dataä¸­éšæœºåˆ’åˆ†70%train/20%val/10%test
    """
    # æ­¥éª¤1ï¼šå°†val/testç›®å½•ä¸‹çš„æ–‡ä»¶ç§»å›dataç›®å½•
    for split in [VAL_DIR, TEST_DIR]:
        for sub_dir in ["images", "masks"]:
            src_dir = os.path.join(split, sub_dir)
            dst_dir = os.path.join(DATA_DIR, sub_dir)
            if os.path.exists(src_dir):
                for file in os.listdir(src_dir):
                    src_path = os.path.join(src_dir, file)
                    dst_path = os.path.join(dst_dir, file)
                    if os.path.exists(dst_path):
                        os.remove(dst_path)
                    shutil.move(src_path, dst_path)
                # æ¸…ç©ºå­ç›®å½•
                shutil.rmtree(src_dir)
    # exit()
    # æ­¥éª¤2ï¼šè·å–dataç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡-æ©ç é…å¯¹
    img_dir = os.path.join(DATA_DIR, "images")
    mask_dir = os.path.join(DATA_DIR, "masks")

    img_extensions = [".jpg", ".jpeg", ".png", ".bmp"]
    img_names = [f for f in os.listdir(img_dir) if any(f.lower().endswith(ext) for ext in img_extensions)]
    random.seed(RANDOM_SEED)
    random.shuffle(img_names)

    # æ­¥éª¤3ï¼šåˆ’åˆ†æ•°æ®é›†ï¼ˆ70%train, 20%val, 10%testï¼‰
    # å…ˆåˆ†train+val(90%) å’Œ test(10%)
    train_val_imgs, test_imgs = train_test_split(img_names, test_size=0.1, random_state=RANDOM_SEED)
    # å†ä»train+valä¸­åˆ†train(70%) å’Œ val(20%)
    train_imgs, val_imgs = train_test_split(train_val_imgs, test_size=2 / 9, random_state=RANDOM_SEED)

    # æ­¥éª¤4ï¼šå¤åˆ¶æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•
    def copy_files(img_list, split_dir):
        img_dst = os.path.join(split_dir, "images")
        mask_dst = os.path.join(split_dir, "masks")
        os.makedirs(img_dst, exist_ok=True)
        os.makedirs(mask_dst, exist_ok=True)

        for img_name in img_list:
            # å¤åˆ¶å›¾ç‰‡
            src_img = os.path.join(img_dir, img_name)
            dst_img = os.path.join(img_dst, img_name)
            shutil.copy(src_img, dst_img)

            # å¤åˆ¶æ©ç ï¼ˆæ”¯æŒ_mask.png/_mask2.pngï¼‰
            img_base = os.path.splitext(img_name)[0]
            mask_candidates = [f"{img_base}_mask.png", f"{img_base}_mask2.png"]
            for mask_name in mask_candidates:
                src_mask = os.path.join(mask_dir, mask_name)
                if os.path.exists(src_mask):
                    dst_mask = os.path.join(mask_dst, mask_name)
                    shutil.copy(src_mask, dst_mask)
                    break

    copy_files(train_imgs, TRAIN_DIR)
    copy_files(val_imgs, VAL_DIR)
    copy_files(test_imgs, TEST_DIR)

    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼")
    print(f"è®­ç»ƒé›†ï¼š{len(train_imgs)} å¼  | éªŒè¯é›†ï¼š{len(val_imgs)} å¼  | æµ‹è¯•é›†ï¼š{len(test_imgs)} å¼ ")


def save_weight_with_time(model, val_loss, optimizer=None):
    """ä¿å­˜float16åŠç²¾åº¦æƒé‡ï¼Œå¤§å¹…å‡å°ä½“ç§¯"""
    os.makedirs(WEIGHT_DIR, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    loss_str = f"{val_loss:.4f}"
    weight_name = f"{timestamp}_best_loss_{loss_str}_fp16.pth"
    weight_path = os.path.join(WEIGHT_DIR, weight_name)

    # å°†æ¨¡å‹å‚æ•°è½¬ä¸ºfloat16
    model_fp16_state_dict = {k: v.half() for k, v in model.state_dict().items()}
    torch.save(model_fp16_state_dict, weight_path, _use_new_zipfile_serialization=True)

    print(f"ğŸ’¾ Float16åŠç²¾åº¦æƒé‡å·²ä¿å­˜è‡³ï¼š{weight_path}")
    return weight_path


# ====================== 3. è‡ªå®šä¹‰æ•°æ®é›†ç±» ======================
class MetSegDataset(Dataset):
    def __init__(self, split="train", transform=None):
        self.split = split
        self.transform = transform
        self.split_dir = {
            "train": TRAIN_DIR,
            "val": VAL_DIR,
            "test": TEST_DIR
        }[split]

        self.img_dir = os.path.join(self.split_dir, "images")
        self.mask_dir = os.path.join(self.split_dir, "masks")
        self.pairs = self._build_pairs()

    def _build_pairs(self):
        """åŒ¹é…åŸå›¾å’Œæ©ç """
        pairs = []
        img_extensions = [".jpg", ".jpeg", ".png", ".bmp"]

        for img_file in os.listdir(self.img_dir):
            if not any(img_file.lower().endswith(ext) for ext in img_extensions):
                continue

            img_base = os.path.splitext(img_file)[0]
            mask_file = None
            for suffix in ["_mask.png", "_mask2.png"]:
                candidate = os.path.join(self.mask_dir, img_base + suffix)
                if os.path.exists(candidate):
                    mask_file = candidate
                    break

            if mask_file:
                pairs.append((
                    os.path.join(self.img_dir, img_file),
                    mask_file,
                    img_base
                ))

        return pairs

    def _process_mask(self, mask_path):
        """å¤„ç†é€æ˜é€šé“/é»‘ç™½æ©ç """
        mask = Image.open(mask_path)
        if mask.mode == "RGBA":
            mask = mask.split()[-1]  # æå–Alphaé€šé“
        elif mask.mode == "RGB":
            mask = mask.convert("L")
        elif mask.mode != "L":
            mask = mask.convert("L")

        mask_np = np.array(mask)
        mask_np = (mask_np > MASK_THRESHOLD).astype(np.uint8)
        return Image.fromarray(mask_np)

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        img_path, mask_path, img_base = self.pairs[idx]

        img = Image.open(img_path).convert("RGB")
        original_size = img.size
        mask = self._process_mask(mask_path)

        if self.transform:
            img = self.transform(img)
            mask_transform = transforms.Compose([
                ResizeWithPad(TARGET_SIZE),
                transforms.ToTensor()
            ])
            mask = mask_transform(mask)
            mask = (mask > 0).float()

        return {
            "image": img,
            "mask": mask,
            "img_base": img_base,
            "original_size": original_size,
            "img_path": img_path
        }


# ====================== 4. è®­ç»ƒ/éªŒè¯/æµ‹è¯•å‡½æ•° ======================
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0.0
    pbar = tqdm(loader, desc="è®­ç»ƒ")

    for batch in pbar:
        imgs = batch["image"].to(device)
        masks = batch["mask"].to(device)

        outputs = model(imgs)
        loss = criterion(outputs, masks)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        pbar.set_postfix({"batch_loss": f"{loss.item():.4f}"})

    return total_loss / len(loader)


def validate(model, loader, criterion, device):
    model.eval()
    total_loss = 0.0
    pbar = tqdm(loader, desc="éªŒè¯")

    with torch.no_grad():
        for batch in pbar:
            imgs = batch["image"].to(device)
            masks = batch["mask"].to(device)

            outputs = model(imgs)
            loss = criterion(outputs, masks)

            total_loss += loss.item()
            pbar.set_postfix({"val_loss": f"{loss.item():.4f}"})

    return total_loss / len(loader)


def infer_single_image(img_path, model, save_dir, device=DEVICE, threshold=0.5):
    """å•å›¾æ¨ç†å¹¶ä¿å­˜æ©ç """
    # åŠ è½½åŸå›¾
    img = Image.open(img_path).convert("RGB")
    original_size = img.size
    img_base = os.path.splitext(os.path.basename(img_path))[0]

    # é¢„å¤„ç†
    transform = transforms.Compose([
        ResizeWithPad(TARGET_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    img_tensor = transform(img).unsqueeze(0).to(device)

    # æ¨ç†
    model.eval()
    with torch.no_grad():
        output = model(img_tensor)
        pred_mask = (output > threshold).float().squeeze(0).squeeze(0).cpu().numpy()
        pred_mask = (pred_mask * 255).astype(np.uint8)

    # è¿˜åŸåŸå›¾å°ºå¯¸
    pred_mask_img = Image.fromarray(pred_mask)
    target_w, target_h = TARGET_SIZE
    original_w, original_h = original_size
    scale = min(target_w / original_w, target_h / original_h)
    new_w = int(original_w * scale)
    new_h = int(original_h * scale)

    pad_left = (target_w - new_w) // 2
    pad_top = (target_h - new_h) // 2
    pred_mask_img = pred_mask_img.crop((pad_left, pad_top, pad_left + new_w, pad_top + new_h))

    pred_mask_img = pred_mask_img.resize(original_size, Image.NEAREST)

    # ä¿å­˜æ©ç 
    save_path = os.path.join(save_dir, f"{img_base}_pred_mask.png")
    pred_mask_img.save(save_path)
    return save_path


def test_model(model, weight_path, test_loader, device=DEVICE):
    """æµ‹è¯•æ¨¡å¼ï¼šåŠ è½½æƒé‡ï¼Œå¯¹æµ‹è¯•é›†æ‰€æœ‰å›¾ç‰‡æ¨ç†"""
    # åŠ è½½æƒé‡
    model.load_state_dict(torch.load(weight_path, weights_only=True))
    print(f"âœ… åŠ è½½æƒé‡å®Œæˆï¼š{weight_path}")
    if os.path.exists(TEST_OUT_DIR):
        shutil.rmtree(TEST_OUT_DIR)
        print('ä»¥å¾€æµ‹è¯•ç»“æœå·²åˆ é™¤')
    # åˆ›å»ºæµ‹è¯•ç»“æœä¿å­˜ç›®å½•
    os.makedirs(TEST_OUT_DIR, exist_ok=True)

    # å¯¹æµ‹è¯•é›†é€å›¾æ¨ç†
    print("\nğŸ“ å¼€å§‹å¯¹æµ‹è¯•é›†æ¨ç†...")
    pbar = tqdm(test_loader, desc="æµ‹è¯•æ¨ç†")
    for batch in pbar:
        img_paths = batch["img_path"]
        for img_path in img_paths:
            infer_single_image(img_path, model, TEST_OUT_DIR, device)

    print(f"âœ… æµ‹è¯•å®Œæˆï¼æ‰€æœ‰æ©ç å·²ä¿å­˜è‡³ï¼š{TEST_OUT_DIR}")


# ====================== 5. ä¸»æµç¨‹ ======================
if __name__ == "__main__":
    # åˆå§‹åŒ–ç›®å½•
    create_dirs()

    # ====================== è®­ç»ƒæ¨¡å¼ ======================
    if MODE == "train":
        # 1. åˆ’åˆ†æ•°æ®é›†ï¼ˆä»dataç›®å½•æ‹†åˆ†train/val/testï¼‰
        split_dataset_from_data()

        # 2. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        train_transform = transforms.Compose([
            ResizeWithPad(TARGET_SIZE),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        val_test_transform = transforms.Compose([
            ResizeWithPad(TARGET_SIZE),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        train_dataset = MetSegDataset(split="train", transform=train_transform)
        val_dataset = MetSegDataset(split="val", transform=val_test_transform)
        test_dataset = MetSegDataset(split="test", transform=val_test_transform)

        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

        print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡ï¼š")
        print(f"è®­ç»ƒé›†ï¼š{len(train_dataset)} æ ·æœ¬ | éªŒè¯é›†ï¼š{len(val_dataset)} æ ·æœ¬ | æµ‹è¯•é›†ï¼š{len(test_dataset)} æ ·æœ¬")

        # 3. åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
        model = ResNetUNet(n_channels=3, n_classes=1).to(DEVICE)
        criterion = nn.BCELoss()
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)

        # 4. å¼€å§‹è®­ç»ƒï¼ˆä»…ä¿å­˜æŸå¤±æ›´å°çš„æƒé‡ï¼‰
        best_val_loss = float('inf')  # åˆå§‹åŒ–æœ€ä¼˜æŸå¤±ä¸ºæ— ç©·å¤§
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒResNet-Unetï¼ˆè®¾å¤‡ï¼š{DEVICE}ï¼‰")
        for epoch in range(EPOCHS):
            print(f"\n===== Epoch {epoch + 1}/{EPOCHS} =====")

            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)
            val_loss = validate(model, val_loader, criterion, DEVICE)

            scheduler.step(val_loss)
            print(f"ğŸ“ˆ è®­ç»ƒæŸå¤±ï¼š{train_loss:.4f} | éªŒè¯æŸå¤±ï¼š{val_loss:.4f}")

            # ä»…å½“å½“å‰éªŒè¯æŸå¤±å°äºå†å²æœ€ä¼˜æŸå¤±æ—¶ï¼Œæ‰ä¿å­˜æƒé‡
            if val_loss < best_val_loss and epoch >= 12:
                best_val_loss = val_loss  # æ›´æ–°æœ€ä¼˜æŸå¤±
                save_weight_with_time(model, best_val_loss, optimizer)  # ä¿å­˜æœ€ä¼˜æƒé‡

        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼ä»…ä¿å­˜äº†éªŒè¯æŸå¤±æœ€å°çš„æœ€ä¼˜æ¨¡å‹è‡³weightç›®å½•")

    # ====================== æµ‹è¯•æ¨¡å¼ ======================
    elif MODE == "test":
        # ä»testæ¨¡å—å¯¼å…¥å¹¶è¿è¡Œæµ‹è¯•
        from utils import run_test

        run_test(
            weight_path=TEST_WEIGHT_PATH,
            test_out_dir=TEST_OUT_DIR,
            batch_size=BATCH_SIZE
        )

    else:
        print(f"âŒ æ— æ•ˆçš„modeå€¼ï¼š{MODE}ï¼Œè¯·è®¾ç½®ä¸º'train'æˆ–'test'")
